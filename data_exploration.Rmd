---
title: "Data Exploration of Tweets"
author: "Yuki Nagae"
output: html_document
---

# Data Exploration of Tweets

## Load Dataset

```{r setup, include=FALSE}
library(tidytext)
library(dplyr)
library(stringr)
library(wordcloud)
library(ggplot2)
```

```{r}
path <- "./python/"
train <- read.delim(paste0(path, "train.tsv"), header = FALSE, sep = " ")
```

```{r}
train_df <- data_frame(word = train$V1, type = train$V52)
train_df <- train_df %>% mutate(tweet = cumsum(str_detect(word, regex("^BOS"))))
```

## Data Exploration

### Identification of types

| Attribute  | Type     | Measured Value               | Description           |
|:-----------|:---------|:-----------------------------|:----------------------|
| word       | nominal  | BOS, @foo, they, will, be, all, done, by, Sunday, EOS | tokenized words and special words (BOS = beginning of sentence, EOS = end of sentence)
| NER type   | nominal  | O (other), B-person, I-person, B-geo-loc, I-geo-loc, B-other, I-other, B-company, I-company, B-facility, I-facility, B-product, I-product, B-musicartist, I-musicartist, B-sportsteam, I-sportsman, B-tvshow, I-tvshow, B-moview, I-movie, OBOS | named entities


### Identification of summarising properties (histgram, box plot etc)

the count the number of types

```{r}
train_df %>% count(type, sort=TRUE)
```

ggplot for the count of the number of types (very sparse)

```{r}
train_df %>% count(type, sort=TRUE) %>%
             mutate(type = reorder(type, n)) %>%
             ggplot(aes(type, n)) + geom_col() + coord_flip()
```

wordcloud visualization

```{r}
train_df %>% count(type) %>% with(wordcloud(type, n, max.words = 100))
```


### Optional
* how many tweets?

```{r}
max(train_df$tweet)
```

* how many words for each tweet?

```{r}
train_df %>% group_by(tweet) %>% summarize(words = n())
```

```{r}
summary(train_df %>% group_by(tweet) %>% summarize(words = n()))
```

* top 30 words

```{r}
train_df %>% count(word, sort=TRUE) %>%
             filter(word != 'BOS') %>%
             filter(word != 'EOS') %>%
             top_n(30) %>%
             mutate(word = reorder(word, n)) %>%
             ggplot(aes(word, n)) + geom_col() + coord_flip()
```

## Named Entities

* Person
	+ B-person
	+ I-person
* Location
	+ B-geo-loc
	+ I-geo-loc
* Organization
	+ B-company
	+ I-company
	+ B-facility (optional)
	+ I-facility (optional)

## Modeling Techniques

* n-grams
* GloVe
* SVM
* KNN

### GloVe + SVM + unigram

setup

```{r}
library(e1071)
library(caret)
```

split data into `train` and `test`

```{r}
train$V1 <- NULL
train_ids <- sample(nrow(train), nrow(train)*0.7)
svm.train <- train[train_ids,] # 70%
svm.test <- train[-train_ids,] # 30%
```

SVM model

```{r}
svm_model <- svm(V252 ~ ., data=svm.train)
svm.predict <- predict(svm_model, svm.test)
```

confusion matrix

```{r}
confusionMatrix(svm.predict, svm.test$V52)
confusionMatrix(svm.predict, svm.test$V52)
```

write results to a file

```{r}
write.table(paste(svm.test$V252, svm.predict, sep = "\t"), file = "glove.twitter.5gram.svm.results", quote = FALSE, row.names = FALSE, col.names = FALSE)
```

## Evaluation

* [CoNLL](http://www.cnts.ua.ac.be/conll2002/ner/) - conlleval.pl

```{sh}
#cat glove.svm.results | tr '\t' ' ' | perl -ne '{chomp;s/\r//g;print $_,"\n";}' | perl connlleval.pl
```

## References

* [Text Mining with R](http://tidytextmining.com)
* GitHub [twitter_nlp](https://github.com/aritter/twitter_nlp)
